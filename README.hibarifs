
CAUTION: Please first check the README file contained in the same
directory as this file for general information on the cluster tool and
how to setup a Hibari cluster.

Your HibariFS installer node must have following tools/environments
ready.  For further information and help for related tools, please
refer to the following links:

- Bash - http://www.gnu.org/software/bash/
- Expect - http://www.nist.gov/el/msid/expect.cfm
- Git - http://git-scm.com/
  * *Git 1.5.4 or newer*
  * _required for GitHub_
- Perl - http://www.perl.org/
- SSH (client) - http://www.openssh.com/

Your HibariFS target nodes must have following tools/environments
ready.  For further information and help for related tools, please
refer to the following links:

- SSH (server) - http://www.openssh.com/

So far, there are no "known" version requirements for Bash, Expect,
Perl, and SSH.



To Setup a HibariFS Cluster
=========================

Prerequisites and Assumptions:

A. 1 installer node
  * Bash, Expect, Git, Perl, and SSH (client) is installed on
    installer node
  * Your login account ($USER) exists on installer node with ssh
    private/public keys and ssh agent setup to enable password-less
    ssh login
  * /etc/hosts file on installer node contains entries for all target
    nodes, Hibari admin nodes, and Hibari brick nodes

B. 1 or more cluster target nodes (e.g. dev1, dev2, dev3)
  * Your login account ($USER) exists on target nodes
  * Your login account ($USER) is enabled with password-less sudo
    access on target nodes
  * Your login account ($USER) is accessible with password-less ssh
    login on target nodes

  * SSH (server) is installed on target nodes
  * /etc/hosts file on target nodes contains entries for all target
    nodes, Hibari admin nodes, and Hibari brick nodes

C. Cluster configuration file. This will show up as hibarifs.config in
   latter explanation.  You have to manually create it on installer
   node and later provide it's location as an input to the cluster
   tool.

  * Hibari Admin nodes
  * HibariFS Client nodes

D. Hibari cluster
  * Hibari cluster was setup and is running according to the procedure
    described in the previously mentioned README file.

Example configuration file (hibarifs.config) for a three node cluster
that uses the same physical nodes as Hibari:

------
ADMIN_NODES=(dev1 dev2 dev3)
CLIENT_NODES=(dev1 dev2 dev3)
------

Example /etc/hosts file entries for above configuration:

------
10.181.165.230  dev1.your-domain.com    dev1
10.181.165.231  dev2.your-domain.com    dev2
10.181.165.232  dev3.your-domain.com    dev3
------


Instructions:

Example how to prepare installing user
======================================

Setup your user (i.e. your login - $USER) on all HibariFS nodes if not
already existing.  This user will only be used for HibariFS
installation purposes.

1. As root user, add your user to all of the HibariFS nodes and grant
   sudo access for your user.

   ------
   $ useradd $USER
   $ passwd $USER
   $ visudo
     # append the following line and save it
     $USER  ALL=(ALL)       NOPASSWD: ALL
   ------

   NOTE: If you get "sudo: sorry, you must have a tty to run sudo"
   error while testing 'sudo', consider to comment out following line
   inside of the /etc/sudoers file:

   ------
   $ visudo
     Defaults    requiretty
   ------


2. Create a new ssh private/public key for your user on the installer
   node.

   ------
   $ ssh-keygen
     # enter your password for the private key
   $ eval `ssh-agent`
   $ ssh-add ~/.ssh/id_rsa
     # re-enter your password for the private key
   ------


3. Append an entry for the installer node to your ~/.ssh/known_hosts
   file on each of the HibariFS nodes and append an entry to your
   ~/.ssh/authorized_keys file on all of the HibariFS nodes for your
   public ssh key.

   ------
   $ ssh-copy-id -i ~/.ssh/id_rsa.pub $USER@dev1
   $ ssh-copy-id -i ~/.ssh/id_rsa.pub $USER@dev2
   $ ssh-copy-id -i ~/.ssh/id_rsa.pub $USER@dev3
   ------

   NOTE: If your installer node will be one of the HibariFS cluster
   nodes, make sure that you ssh-copy-id to the installer node also.


4. Confirm password-less access to the each of the HibariFS nodes
   works as expected.

   ------
   $ ssh $USER@dev1
   $ ssh $USER@dev2
   $ ssh $USER@dev3
   ------


TIP: If needed, check
http://inside.mines.edu/~gmurray/HowTo/sshNotes.html for further SSH
setup help.



Example how to prepare installer node
=====================================

1. Configure your e-mail and name for Git

   ------
   $ git config --global user.email "you@example.com"
   $ git config --global user.name "Your Name"
   ------


2. Create a working directory

   ------
   $ mkdir working-directory-name
   ------


3. Download cluster tool's Git repository

   ------
   $ cd working-directory-name
   $ git clone git://github.com/hibari/clus.git
   ------


4. Place a copy of the HibariFS pre-built release and your
   hibarifs.config file into the working directory.

   ------
   $ cd working-directory-name
   $ ls -1
   clus
   hibarifs-X.Y.Z-DIST-ARCH-WORDSIZE-md5sum.txt
   hibarifs-X.Y.Z-DIST-ARCH-WORDSIZE.tgz
   hibarifs.config
   $
   ------


Example how to create all HibariFS nodes
========================================

All of the operations below are run on the installer node via two Bash
scripts (i.e. clus.sh and clus-hibarifs.sh).

1. Create (or re-create) "hibarifs" user on all HibariFS nodes

   NOTE: If your ssh private key is protected by a password, please
   make sure your private key is registered with the ssh agent before
   proceeding.

   ------
   $ cd working-directory-name
   $ for i in dev1 dev2 dev3 ; do ./clus/priv/clus.sh -f init hibarifs $i ; done
   hibarifs@dev1
   hibarifs@dev2
   hibarifs@dev3
   ------

   CAUTION: The -f option will forcefully delete and then re-create
   the "hibarifs" user on the target node.


2. Copy pre-built release to all HibariFS nodes and then setup
   HibariFS package on all HibariFS nodes via the "hibarifs" user.
   The last argument is the name of the "hibari" user used for the
   running Hibari cluster.

   ------
   $ cd working-directory-name
   $ ./clus/priv/clus-hibarifs.sh -f init hibarifs hibarifs.config hibarifs-X.Y.Z-DIST-ARCH-WORDSIZE.tgz hibari
   hibarifs@dev1
   hibarifs@dev2
   hibarifs@dev3
   ------


3. Start HibariFS on all HibariFS nodes via the "hibarifs" user

   ------
   $ cd working-directory-name
   $ ./clus/priv/clus-hibarifs.sh -f start hibarifs hibarifs.config
   hibarifs@dev1
   hibarifs@dev2
   hibarifs@dev3
   ------


4. Bootstrap HibariFS on first HibariFS admin node via the "hibari"
   user

    ------
    $ cd working-directory-name
    $ ./clus/priv/clus-hibarifs.sh -f bootstrap hibarifs hibarifs.config hibari
    ok
    [hibarifs@dev3,hibarifs@dev2,hibarifs@dev1]
    ------

    The HibariFS bootstrap process registers all of the HibariFS nodes
    as clients with Hibari's admin server.


5. Open "HibariFS Web Administration" page and manually confirm each
   of the HibariFS nodes is registered via the "Add/Delete a client
   node monitor." link.  Next, ping each of the nodes to check the
   health.

    ------
    $ your-favorite-browser http://dev1:23080
    ------

    ------
    $ cd working-directory-name
    $ ./clus/priv/clus-hibarifs.sh -f ping hibarifs hibarifs.config
    hibarifs@dev1 ... pong
    hibarifs@dev2 ... pong
    hibarifs@dev3 ... pong
    ------


6. Mount HibariFS on all HibariFS nodes via the "hibarifs" user

    ------
    $ cd working-directory-name
    $ ./clus/priv/clus-hibarifs.sh -f mount hibarifs hibarifs.config
    ok
    ok
    ok
    hibarifs@dev1
    hibarifs@dev2
    hibarifs@dev3
    ------


7. Umount HibariFS on all HibariFS nodes via the "hibarifs" user

    ------
    $ cd working-directory-name
    $ ./clus/priv/clus-hibarifs.sh -f umount hibarifs hibarifs.config
    ok
    ok
    ok
    hibarifs@dev1
    hibarifs@dev2
    hibarifs@dev3
    ------


8. Stop HibariFS on all HibariFS nodes via the "hibarifs" user

    ------
    $ cd working-directory-name
    $ ./clus/priv/clus-hibarifs.sh -f stop hibarifs hibarifs.config
    ok
    ok
    ok
    hibarifs@dev1
    hibarifs@dev2
    hibarifs@dev3
    ------


TIP: The clus-hibarifs.sh script can be used for starting, mounting,
umounting, and stopping of a HibariFS cluster even after it's
creation.


Other Considerations
====================

Depending on deployment and usage of HibariFS, additional FUSE and
Hibari related configuration items might be required.

*FUSE*

A common configuration scenario is to allow other users to access the
mounted filesystem.  The /etc/fuse.conf file should be changed as
follows:

    ------
    $ cat /etc/fuse.conf
    user_allow_other
    ------

Similiarly, the HibariFS "hibarifs_fuse" application's configuration
for the mount options parameter should be set to "allow_other".  This
configuration is contained in the example etc/app.config file below.

*HIBARI*

A common configuration item is to specify the Hibari table used by the
HibariFS client.  The name of the table and it's corresponding
varprefixnum value should be set to "mount_table" and
"mount_varprefixnum", respectively.  This configuration is contained
in the example etc/app.config file below.

*EXAMPLE etc/app.config file*

    ------
    %%
    %% HibariFS config
    %%
    hibarifs_fuse,
    [{linked_in, false}
     , {mount_point, "/mnt/tab1"}
     , {mount_opts, "allow_other"}
     , {make_mount_point, true}
     , {mount_table, tab1}
     , {mount_varprefixnum, 2}
     , {mount_timeout, 5000}
     , {dirmode, 8#00777}
     , {filemode, 8#00666}
     , {delayed_write, true}
     , {attr_timeout, 1000}
     , {entry_timeout, 1000}
     , {cache_timeout, 5000}
    ]}
    ------


Operating System Specifics - CentOS (5.5)
=========================================

HibariFS is implemented as a FUSE fileystem.  The following steps are
required to setup FUSE for CentOS.

    ------
    $ modprobe fuse
    $ chmod go+rw /dev/fuse
    $ yum install fuse fuse-devel fuse-libs
    $ chmod +x /bin/fusermount
    ------

CAUTION: kernel 2.6.18-164 version or newer is required for FUSE and
HibariFS.
