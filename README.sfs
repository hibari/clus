
CAUTION: Please first check the README file contained in the same
directory as this file for general information on the cluster tool and
how to setup a Hibari cluster.

Your SFS installer node must have following tools/environments
ready.  For further information and help for related tools, please
refer to the following links:

- Bash - http://www.gnu.org/software/bash/
- Expect - http://www.nist.gov/el/msid/expect.cfm
- Git - http://git-scm.com/
  * *Git 1.5.4 or newer*
  * _required for GitHub_
- Perl - http://www.perl.org/
- SSH (client) - http://www.openssh.com/

Your SFS target nodes must have following tools/environments
ready.  For further information and help for related tools, please
refer to the following links:

- SSH (server) - http://www.openssh.com/

So far, there are no "known" version requirements for Bash, Expect,
Perl, and SSH.



To Setup a SFS Cluster
=========================

Prerequisites and Assumptions:

A. 1 installer node
  * Bash, Expect, Git, Perl, and SSH (client) is installed on
    installer node
  * Your login account ($USER) exists on installer node with ssh
    private/public keys and ssh agent setup to enable password-less
    ssh login
  * /etc/hosts file on installer node contains entries for all target
    nodes, Hibari admin nodes, and Hibari brick nodes

B. 1 or more cluster target nodes (e.g. dev1, dev2, dev3)
  * Your login account ($USER) exists on target nodes
  * Your login account ($USER) is enabled with password-less sudo
    access on target nodes
  * Your login account ($USER) is accessible with password-less ssh
    login on target nodes

  * SSH (server) is installed on target nodes
  * /etc/hosts file on target nodes contains entries for all target
    nodes, Hibari admin nodes, and Hibari brick nodes

C. Cluster configuration file. This will show up as sfs.config in
   latter explanation.  You have to manually create it on installer
   node and later provide it's location as an input to the cluster
   tool.

  * Hibari Admin nodes
  * SFS Client nodes

D. Hibari cluster
  * Hibari cluster was setup and is running according to the procedure
    described in the previously mentioned README file.

Example configuration file (sfs.config) for a three node cluster
that uses the same physical nodes as Hibari:

------
ADMIN_NODES=(dev1 dev2 dev3)
CLIENT_NODES=(dev1 dev2 dev3)
------

Example /etc/hosts file entries for above configuration:

------
10.181.165.230  dev1.your-domain.com    dev1
10.181.165.231  dev2.your-domain.com    dev2
10.181.165.232  dev3.your-domain.com    dev3
------


Instructions:

Example how to prepare installing user
======================================

Setup your user (i.e. your login - $USER) on all SFS nodes if not
already existing.  This user will only be used for SFS
installation purposes.

1. As root user, add your user to all of the SFS nodes and grant
   sudo access for your user.

   ------
   $ useradd $USER
   $ passwd $USER
   $ visudo
     # append the following line and save it
     $USER  ALL=(ALL)       NOPASSWD: ALL
   ------

   NOTE: If you get "sudo: sorry, you must have a tty to run sudo"
   error while testing 'sudo', consider to comment out following line
   inside of the /etc/sudoers file:

   ------
   $ visudo
     Defaults    requiretty
   ------


2. Create a new ssh private/public key for your user on the installer
   node.

   ------
   $ ssh-keygen
     # enter your password for the private key
   $ eval `ssh-agent`
   $ ssh-add ~/.ssh/id_rsa
     # re-enter your password for the private key
   ------


3. Append an entry for the installer node to your ~/.ssh/known_hosts
   file on each of the SFS nodes and append an entry to your
   ~/.ssh/authorized_keys file on all of the SFS nodes for your
   public ssh key.

   ------
   $ ssh-copy-id -i ~/.ssh/id_rsa.pub $USER@dev1
   $ ssh-copy-id -i ~/.ssh/id_rsa.pub $USER@dev2
   $ ssh-copy-id -i ~/.ssh/id_rsa.pub $USER@dev3
   ------

   NOTE: If your installer node will be one of the SFS cluster
   nodes, make sure that you ssh-copy-id to the installer node also.


4. Confirm password-less access to the each of the SFS nodes
   works as expected.

   ------
   $ ssh $USER@dev1
   $ ssh $USER@dev2
   $ ssh $USER@dev3
   ------


TIP: If needed, check
http://inside.mines.edu/~gmurray/HowTo/sshNotes.html for further SSH
setup help.



Example how to prepare installer node
=====================================

1. Configure your e-mail and name for Git

   ------
   $ git config --global user.email "you@example.com"
   $ git config --global user.name "Your Name"
   ------


2. Create a working directory

   ------
   $ mkdir working-directory-name
   ------


3. Download cluster tool's Git repository

   ------
   $ cd working-directory-name
   $ git clone git://github.com/hibari/clus.git
   ------


4. Place a copy of the SFS pre-built release and your
   sfs.config file into the working directory.

   ------
   $ cd working-directory-name
   $ ls -1
   clus
   sfs-X.Y.Z-DIST-ARCH-WORDSIZE-md5sum.txt
   sfs-X.Y.Z-DIST-ARCH-WORDSIZE.tgz
   sfs.config
   $
   ------


Example how to create all SFS nodes
========================================

All of the operations below are run on the installer node via two Bash
scripts (i.e. clus.sh and clus-sfs.sh).

1. Create (or re-create) "sfs" user on all SFS nodes

   NOTE: If your ssh private key is protected by a password, please
   make sure your private key is registered with the ssh agent before
   proceeding.

   ------
   $ cd working-directory-name
   $ for i in dev1 dev2 dev3 ; do ./clus/priv/clus.sh -f init sfs $i ; done
   sfs@dev1
   sfs@dev2
   sfs@dev3
   ------

   CAUTION: The -f option will forcefully delete and then re-create
   the "sfs" user on the target node.


2. Copy pre-built release to all SFS nodes and then setup
   SFS package on all SFS nodes via the "sfs" user.
   The last argument is the name of the "hibari" user used for the
   running Hibari cluster.

   ------
   $ cd working-directory-name
   $ ./clus/priv/clus-sfs.sh -f init sfs sfs.config sfs-X.Y.Z-DIST-ARCH-WORDSIZE.tgz hibari
   sfs@dev1
   sfs@dev2
   sfs@dev3
   ------


3. Start SFS on all SFS nodes via the "sfs" user

   ------
   $ cd working-directory-name
   $ ./clus/priv/clus-sfs.sh -f start sfs sfs.config
   sfs@dev1
   sfs@dev2
   sfs@dev3
   ------


4. Bootstrap SFS on first SFS admin node via the "hibari"
   user

    ------
    $ cd working-directory-name
    $ ./clus/priv/clus-sfs.sh -f bootstrap sfs sfs.config hibari
    ok
    [sfs@dev3,sfs@dev2,sfs@dev1]
    ------

    The SFS bootstrap process registers all of the SFS nodes
    as clients with Hibari's admin server.


5. Open "SFS Web Administration" page and manually confirm each
   of the SFS nodes is registered via the "Add/Delete a client
   node monitor." link.  Next, ping each of the nodes to check the
   health.

    ------
    $ your-favorite-browser http://dev1:23080
    ------

    ------
    $ cd working-directory-name
    $ ./clus/priv/clus-sfs.sh -f ping sfs sfs.config
    sfs@dev1 ... pong
    sfs@dev2 ... pong
    sfs@dev3 ... pong
    ------


6. Mount SFS on all SFS nodes via the "sfs" user

    ------
    $ cd working-directory-name
    $ ./clus/priv/clus-sfs.sh -f mount sfs sfs.config
    ok
    ok
    ok
    sfs@dev1
    sfs@dev2
    sfs@dev3
    ------


7. Umount SFS on all SFS nodes via the "sfs" user

    ------
    $ cd working-directory-name
    $ ./clus/priv/clus-sfs.sh -f umount sfs sfs.config
    ok
    ok
    ok
    sfs@dev1
    sfs@dev2
    sfs@dev3
    ------


8. Stop SFS on all SFS nodes via the "sfs" user

    ------
    $ cd working-directory-name
    $ ./clus/priv/clus-sfs.sh -f stop sfs sfs.config
    ok
    ok
    ok
    sfs@dev1
    sfs@dev2
    sfs@dev3
    ------


TIP: The clus-sfs.sh script can be used for starting, mounting,
umounting, and stopping of a SFS cluster even after it's
creation.


Other Considerations
====================

Depending on deployment and usage of SFS, additional FUSE,
Hibari and Linux Kernel related configuration items might be required.

*FUSE*

A common configuration scenario is to allow other users to access the
mounted filesystem.  The /etc/fuse.conf file should be changed as
follows:

    ------
    $ cat /etc/fuse.conf
    user_allow_other
    ------

Similiarly, the SFS "sfs_fuse" application's configuration
for the mount options parameter should be set to "allow_other".  This
configuration is contained in the example etc/app.config file below.

*HIBARI*

A common configuration item is to specify the Hibari table used by the
SFS client.  The name of the table and it's corresponding
varprefixnum value should be set to "mount_table" and
"mount_varprefixnum", respectively.  This configuration is contained
in the example etc/app.config file below.

*Linux Kernel perameters*

On your SFS node, in the system's /etc/sysctl.conf file, add the
following settings.

    ------
    # Swappiness is not desirable for en Erlnag VM.
    vm.swappiness=0

    # This setting makes the kernel to prefer pagecache over
    # inode/dentry cache. SFS node typically has a large amount
    # of RAM and kernel tends to cache too many inodes with the
    # default value (100). This will lead SFS to create more
    # Erlang processes than it actually needs so it will consume more
    # memory. Having the value 10000 will improve this behavior.
    #
    vm.vfs_cache_pressure=10000

    # Uncomment this if your SFS node has enough RAM and you
    # want to be sure that swapping would not happen.
    # 0: (default) allow over-committing memory
    # 2: don't allow over-ommitting memory
    #
    #vm.overcommit_memory=2

    # You can adjust this setting to limit the total virtual address
    # space on the system. The limit is calicuated by the follwoing
    # formula:
    # swap_space + (RAM * (overcommit_ratio/100))
    #
    #vm.overcommit_ratio=0
    ------

*EXAMPLE etc/app.config file*

    ------
    %%
    %% SFS config
    %%
   {sfs_fuse,
    [{spawn_opts, [{fullsweep_after, 5}]}
     , {mount_point, "/mnt/tab1"}
     , {mount_opts, "allow_other"}
     , {make_mount_point, true}
     , {mount_table, tab1}
     , {mount_varprefixnum, 0}
     , {mount_timeout, 60000}
     , {dirmode, 8#00775}
     , {filemode, 8#00664}
     , {attr_timeout, 1000}
     , {entry_timeout, 1000}
     , {cache_dirtimeout, 12000}
     , {cache_filetimeout, 6000}
     , {cache_dirhibernate, true}
     , {cache_filehibernate, true}
     , {cache_spawn_opts, [{fullsweep_after, 5}]}
    ]}
    ------


Operating System Specifics - CentOS (5.5)
=========================================

SFS is implemented as a FUSE fileystem.  The following steps are
required to setup FUSE for CentOS.

    ------
    $ modprobe fuse
    $ chmod go+rw /dev/fuse
    $ yum install fuse fuse-devel fuse-libs
    $ chmod +x /bin/fusermount
    ------

CAUTION: kernel 2.6.18-164 version or newer is required for FUSE and
SFS.
